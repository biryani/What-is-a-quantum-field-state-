%\documentclass[twocolumn,lengthcheck,superscriptaddress]{revtex4-1}

\documentclass[11pt]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{a4paper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{color}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{epstopdf}
\usepackage{mathrsfs}
\usepackage{enumerate}
\usepackage{xypic}
\usepackage[shortalphabetic]{amsrefs}

\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\vspan}{span}
\DeclareMathOperator{\res}{res}

\theoremstyle{plain}% default
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem*{corollary}{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{example}{Example}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{note}{Note}

%\bibliographystyle{amsalpha}

\title{What is a quantum field state?}
\date{\today}                                           % Activate to display a given date or no date


\begin{document}

\begin{abstract} 
There has recently been in a fruitful interplay of ideas between quantum information theory and high energy physics in the context of the AdS/CFT correspondence and the black hole information loss paradox. However, a core difficulty faced by quantum information theorists -- who are usually concerned with qubits -- interested in these emergent and vibrant areas is the necessity of dealing with quantum fields. The primary purpose of these notes is to lower the entry barrier for quantum information theorists to work on such exciting and challenging topics by explaining what, in a quantum information friendly way, a quantum field state actually is. We describe the Wilsonian formulation of quantum field theory as an effective theory and explain how this leads naturally to a definition of quantum field states independent of lagrangians appropriate for tensor networks. We hope that there is something here for high-energy theorists as well, if only to see how someone from ``the other side'' thinks about complex quantum systems.
\end{abstract}

\maketitle

\section{Introduction}
Quantum field theory (QFT) has become, thanks in no small part to Wilson 
\cite{wilson_renormalization_1974,wilson_renormalization_1975}, an immensely powerful calculational machine to solve and approximate a wide variety of physical problems from the fundamental physics of particles \cite{peskin_introduction_1995,weinberg_quantum_1996,weinberg_quantum_1996-1,weinberg_quantum_2000} to the effective description of many body interacting quantum systems such as magnets and dilute atomic gases \cite{fradkin_field_2013}. Thus it is no real exaggeration to say that QFT is the calculus of modern physics \cite{witten_surface_2006,seiberg_nathan_2014}. In contrast to the mature status that calculus enjoys, however, QFT is still far from a stable formulation \cite{seiberg_nathan_2014} as texts on the subject are not standardised and, further,  mathematicians are not yet universally happy with QFT as practiced by physicists. 

Amongst the many formulations of QFT, a most popular one is in terms of \emph{lagrangians}. Here one begins with a set of \emph{classical} equations of motion, encapsulated by the lagrangian via the principle of least action \cite{arnold_mathematical_1989}, and then one seeks a \emph{quantisation} of these equations of motion, typically via the path integral prescription. The path integral approach is not without problems; nevertheless, it is the approach that has lead to arguably the most progress. For example, it works extremely well in the perturbative setting, where quantum field theory is now on a rather firm footing and, more importantly, it also provides an elegant way to approach the \emph{nonperturbative} setting where, e.g., it serves as the basis of lattice gauge theory \cite{creutz_quarks_1985,wilson_confinement_1974}. In the case of lattice gauge theory a dramatic validation of the path integral approach was recently obtained where the hadronic spectrum of QCD was numerically obtained \cite{durr_ab_2008}. However, despite the power and ubiquity of the lagrangian/path integral approach, there are still many mysteries concerning nonperturbative QFT.  

One way to make progress on understanding nonperturbative QFT might be to go beyond the lagrangian parametrisation. This is not a new idea: in the '60s and '70s the idea of deriving all of physics from the analyticity properties \cite{eden_analytic_2002} of the $S$ matrix was very popular. This idea lost steam in the late '70s in the wake of the stunning success of the standard model. Recently, however, there has been an upswing in interest in formulations of QFT without the lagrangian. A major impetus here comes from  string theory where there are arguments that there exist certain superconformal quantum field theories in five and six dimensions \cite{witten_comments_1995,moore_lecture_2012} with no (known) lagrangian formulation. Additional motivation for studying QFT without lagrangians has been powerfully articulated by Arkani-Hamed and collaborators in the course of the programme to understanding the scattering amplitudes for $\mathcal{N}=4$ supersymmetric Yang-Mills theory \cite{arkani-hamed_what_2010,arkani-hamed_into_2014,arkani-hamed_tree_2008,arkani-hamed_scattering_2012,arkani-hamed_all-loop_2011,arkani-hamed_amplituhedron_2014,arkani-hamed_s-matrix_2010,arkani-hamed_what_2010}: here the core motivation is to find a formulation of QFT which exposes symmetries at the expense of  manifest unitarity and locality, i.e., to allow spacetime to be an emergent property.  

The theme of emergent spacetime plays a crucial role in discussions of the \emph{holographic principle} \cite{bousso_holographic_2002}. In the specific concrete context of the AdS/CFT correspondence \cite{maldacena_large_1999} we have seen that spacetimes of certain associated bulk degrees of freedom are encoded into the hilbert spaces of strongly interacting quantum many body systems living on spatial boundaries. The nature of of this encoding is still not completely understood, particularly away from the large $N$ limit where semiclassical arguments are no longer valid. However, this idea has proved to be so deep that even just taking aim at its general direction has lead to spectacular and exciting progress. Most relevant for this paper is a line of enquiry beginning with the work of Swingle \cite{swingle_entanglement_2012} applying tensor networks to quantify the nature of, and the correspondence between, bulk and boundary degrees of freedom \cite{nozaki_holographic_2012,ryu_aspects_2006,ryu_holographic_2006}. Also closely related, are studies exploiting quantum information theoretic ideas, particularly from the study of quantum error correcting codes \cite{almheiri_bulk_2014} and quantum Shannon theory \cite{czech_information_2014}, aimed at elucidating the interpretation of AdS/CFT duality. These recent studies usually work by first discretisating the problem and then deploying the apparatus of quantum information theory to the resulting discrete system. The precise way in which results obtained in this way survive the limit to the continuum is rather subtle.

Continuing with the themes of holography, emergent spacetime geometry, and quantum information theory, there has been a recent flurry of activity centred around the ``firewall'' paradox, initiated by the work of \cite{almheiri_black_2013}. The ensuing debate has prompted many intriguing and original ideas aimed at resolving the paradox. One extremely suggestive proposal \cite{maldacena_cool_2013}, known as ``ER=EPR'', posits that the fabric of spacetime itself is none other than quantum entanglement. This is heady stuff! However it is hard, especially for the quantum information theorist, to make concrete sense of it, especially since at first sight, the proposal appears to be a category error. 

We believe that quantum information theorists still have interesting things to contribute to high energy physics. We see several possibilities here. Firstly, and most directly, we could develop results which have already proved successful in the study of strongly correlated complex quantum systems to apply to settings of direct interest in high energy physics. In condensed matter physics inspiration from quantum information theory has led to the development of new variational families of \emph{tensor network state} (TNS), including, the \emph{projected entangled pair states} (PEPS) \cite{verstraete_renormalization_2004} and the \emph{multiscale entanglement renormalisation ansatz} (MERA) \cite{vidal_entanglement_2007,vidal_class_2008}. The crucial idea underlying these developments is that TNS provide a parsimonious and expressive \emph{data structure} to parametrise \cite{haegeman_geometry_2014} the hilbert space of states naturally arising \cite{poulin_quantum_2011} in the low-energy sector of local quantum systems \cite{orus_practical_2014}. 

One way to carrying out the goal of understanding quantum fields via tensor network methods is to formulate TNS directly in the continuum. This approach has given rise to the continuous matrix products states (cMPS), continuous PEPS, and continuous MERA classes. Such continuous TNS have already provided some new insights in the study of some problems in high-energy physics, and promise to provide a powerful way to reason about entangled quantum fields. Another way, adopted in this paper, is to simply understand how discrete TNS approximate a given QFT. This approach is easier to implement numerically, and also more directly allows the computation of, e.g., quantum entanglement, but it does throw up the intriguing question of concretely understanding how a discretised TNS has anything to do with a given continuous QFT.  

Another avenue where quantum information theory seems likely to lead to progress in high energy physics is via simulation on a quantum computer. Here discretising a quantum field is the most obvious way forward for simulation on a quantum computer. This is a  goal because quantum computers promise to allow the computation of scattering amplitudes involving many particles which require, even perturbatively, the summation of a prohibitive number of diagrams. Pursuing this idea has led to the development of quantum simulation algorithms for $\phi^4$ theory and the Gross-Neveu model on the lattice. Here there are again fascinating questions about how to understand the nonperturbative regime and the approach to the continuum.  

Finally, and somewhat more speculatively, the work of Arkani-Hamed and collaborators is rather suggestive to someone with a quantum information background. Here it is proposed that scattering amplitudes can be directly obtained from the volumes of a certain special convex set known as the \emph{amplituhedron}. This convex set bears a superficial resemblence to the problem of understanding the convex set of reduced density operators for complex quantum systems. One can argue that scattering amplitudes may be derived from knowledge of this convex set. This topic has connections 

We've structured the notes as follows. We begin with a short overview of operational quantum mechanics with an emphasis on density operators, completely positive maps, and POVMs. After setting up this basic language we move to a short overview of the problems of quantum field theory, including quantisation and the construction of states. With this motivation we review the Wilsonian formulation of effective quantum field theory. This is then used as the motivation for our construction of the quantum field state in the next section. 

So what is a quantum field state? Our answer, as we argue in the course of this paper, is simply that a quantum field state is actually a \emph{sequence of states} of discretised theories with a certain property, namely, that each term gets closer to each other (i.e., a Cauchy sequence). Following in the footsteps of Wilson, we spend some time discussing how to measure ``close'', leading to the introduction of a family of quantum information distance measures quantifying the large-scale behaviour of a many particle state. 

\section{Operational quantum physics}
Throughout these notes we emphasise the \emph{operational viewpoint}: here the focus is on physical quantities with an \emph{operational interpretation}, i.e., a physical quantity is considered operationally meaningful only if there exists, at least in principle, an experiment which could measure it. The operational viewpoint is common within quantum information theory and also seems to mesh rather well with the Wilsonian view of QFT as an effective field theory. 

A convenient way to discuss quantum physics within the operational viewpoint is via \emph{observables and effects}, \emph{density operators}, and \emph{completely positive maps}. This language may be unfamiliar to the reader and we pause a moment to review it here. The way we characterise \emph{quantum} (indeed, also \emph{classical}) systems is via a set $\mathcal{A}$ of \emph{observables}. The observables typically form an algebra isomorphic to the bounded operators $\mathcal{B}(\mathcal{H})$ on some hilbert space $\mathcal{H}$, however this is not strictly necessary\footnote{The algebra structure is usually employed as a proxy for the positivity notion. It also plays a role in specifying dynamics, more on this later.} for there to be a probability interpretation. For the probability interpretation we need only require that $\mathcal{A}$ has a notion of \emph{positivity}, i.e., there is a cone $\mathcal{A}^+ \subset \mathcal{A}$ of positive elements and that there is a distinguished unit element $\mathbb{I}\in\mathcal{A}$ which is also positive. A good example to keep in mind here is that of the \emph{qubit}, where $\mathcal{A}\equiv M_2(\mathbb{C})$, the algebra of $2\times 2$ complex matrices, $\mathcal{A}^+ \equiv \{M\in \mathcal{A}\,|\, M\ge 0\}$ (with $\ge$ denoting the positive semidefinite order, i.e., $M\ge 0$ if and only if there exists $A\in \mathcal{A}$ such that $M = A^\dag A$). Another example is that of a \emph{classical} system, which is characterised by a \emph{commutative algebra} $\mathcal{A}\equiv \mathcal{C}(X)$, the set of functions from some set $X$ to $\mathbb{C}$; a \emph{classical bit} corresponds to the choice $X \equiv \{0,1\}$. An \emph{effect} $E\in\mathcal{A}^+$ is then what we call an observable corresponding to an \emph{outcome}, \emph{proposition}, \emph{predicate}, or \emph{yes/no measurement}. It is characterised by the property that $0\le E\le \mathbb{I}$. The unit element is the effect corresponding to empty predicate, that is, no assertion. For the qubit example above the projector $P_0 = \left(\begin{smallmatrix} 1 & 0 \\ 0 & 0\end{smallmatrix}\right)$ is the effect corresponding to the assertion that the system is in the ``zero'' configuration. A POVM is a set $\{E_j\}_{j=1}^n$ of effects such that 
\begin{equation}
	\sum_{j=1}^n E_j = \mathbb{I}.
\end{equation}

A \emph{state} $\omega:\mathcal{A}\rightarrow \mathbb{C}$ on the observables $\mathcal{A}$ is a \emph{positive}, \emph{normalised}, and \emph{linear} functional, i.e., $\omega:\mathcal{A}^+\rightarrow \mathbb{R}^+$ and $\omega(e) = 1$. A state describes a \emph{preparation} of the system, and captures all the information relevant for the statistical outcomes of measurements on the system. The probability $p_E$ that an outcome with corresponding effect $E$ occurs is given by $p_E = \omega(E)$. Usually in quantum information theory we work with finite-dimensional quantum systems with $\mathcal{A}\equiv M_d(\mathbb{C})$ so that we can represent states via \emph{density operators} $\rho\in M_d(\mathbb{C})$ according to $\omega(M) \equiv \tr(\rho M)$, with $\tr(\rho) = 1$, $\rho \ge 0$. (Be aware that in infinite-dimensional settings it is not always possible to find a density operator corresponding to a state as the trace condition can easily fail.) A state $\omega$ is \emph{pure} if it cannot be written as a convex combination of other states, i.e., if $\omega \not= p \omega' + (1-p) \omega''$, with $p\in (0,1)$. Continuing the qubit example from above we see that single-qubit states $\omega$ correspond to $2\times 2$ density operators 
\begin{equation}
	\rho = \frac{\mathbb{I} + \mathbf{r}\cdot \boldsymbol{\sigma}}{2},  
\end{equation}  
with $\boldsymbol{\sigma} \equiv \left[\left(\begin{smallmatrix} 0 & 1 \\ 1 & 0\end{smallmatrix}\right), \left(\begin{smallmatrix} 0 & -i \\ i & 0\end{smallmatrix}\right), \left(\begin{smallmatrix} 1 & 0 \\ 0 & -1\end{smallmatrix}\right)\right]$. We have that $\omega(\sigma^j) = \tr(\rho \sigma^j)$. The condition that $\rho$ corresponds to a state translates to the condition that $|\mathbf{r}| \le 1$.

Compositions of systems is described via the \emph{tensor product} operation: suppose we have two systems $A$ and $B$ characterised by the observables $\mathcal{A}_A$ and $\mathcal{A}_B$. The joint system $AB$ is characterised by the observables $\mathcal{A}_{AB} \equiv \mathcal{A}_A\otimes \mathcal{A}_B$.


The most general dynamical process that can occur within quantum mechanics is represented by a \emph{completely positive map} (CP map), or \emph{channel}. These processes describe everything from preparations, time evolutions, measurements, and measurements with general state changes. The input and output of a CP map may be arbitrary: there is no especial difficulty in the formulation caused by, e.g., multiple input and output systems. We describe CP maps in the \emph{heisenberg picture} where modifications of measurements are specified. Here a CP map $\mathcal{E}$ is characterised by the following three axioms: 
\begin{enumerate}
	\item it is a \emph{linear} map $\mathcal{E}:\mathcal{A}_B\rightarrow \mathcal{A}_A$ from the observables $\mathcal{A}_A$ of an \emph{output system} to the observables of $\mathcal{A}_A$ of the  \emph{input system};
	\item the map $\mathcal{E}$ is \emph{positive}, i.e., it takes positive elements to positive elements, i.e., $\mathcal{E}:\mathcal{A}_B^+\rightarrow \mathcal{A}_A^+$; and 
	\item running the map $\mathcal{E}$ in parallel with the \emph{identity channel} $\mathcal{I}_E : \mathcal{A}_E\rightarrow \mathcal{A}_E$ on an arbitrary auxiliary system $\mathcal{A}_E$ yields a positive linear map $\mathcal{E}\otimes \mathcal{I}_E$ from $\mathcal{A}_B\otimes \mathcal{A}_E$ to $\mathcal{A}_A\otimes \mathcal{A}_E$.
\end{enumerate} 
Pleasingly this axiomatic characterisation of a general dynamical process matches the following constructive characterisation. Suppose we want to only allow maps which comprise of the following three primitives: (1) adjoining ancillary systems (via the tensor product); (2) unitary transformations; and (3) reduction to a subsystem (partial trace). Then it is a result of Stinespring that such operations correspond precisely to CP maps and vice versa. 

In the qubit example we describe the dynamics of a qubit 

\section{Quantisation isn't a mystery, it's an inverse problem}
Before we get started with understanding quantum field states we pause a moment to stress a simple yet important point. The universe didn't become quantum in 1927 at the Fifth Solvay International Conference, it has always been quantum. The reason that we didn't notice quantum effects for such a long time is because of \emph{decoherence} \cite{joos_decoherence_2003,gardiner_quantum_2010}, i.e., the unavoidable loss of quantum coherence due to uncontrolled interactions with environment degrees of freedom. In the presence of quantum noise pure unitary dynamics described by a unitary channel $\mathcal{U}_t$ is modified to a noisy CP map $\mathcal{E}_t$ which can, for quantum systems with a continuous degree of freedom (e.g., a particle on the line), be \emph{very effectively} modelled by a symplectic transformation on phase space.
\begin{center}
\includegraphics{Decoherence.pdf}
\end{center}
The process of decoherence is complicated and deleterious. The most relevant feature of decoherence for this discussion is that many different quantum dynamical processes can be, after decoherence has set in, effectively modelled by the \emph{same} classical dynamical system. The decoherence map is, amongst other things, a many to one map. Thus, when presented with a given classical dynamical system we have no way, not even in principle, of identifying the ``correct'' quantisation; we are trying to invert a many to one map and there is no canonical choice of preimage.  None of this is really surprising or controversial. (Note that the \emph{semiclassical limit} is \emph{not} the same as the \emph{effective classical limit} arising from the presence of decoherence. While there is, in both cases, an emergence of classicality from a quantum system, the reasoning is totally different.)

Quantisation is hard precisely because we are trying to simultaneously solve more than one problem at once: we are trying to find a quantum system such that when we solve the system in the presence of decoherence, we find it is well described by a classical effective theory which matches the originally specified classical system. Said this way it is truly a miracle that the bewildering variety of quantisation recipes developed over the past decades work at all! 
\begin{center}
\includegraphics{Quantisation.pdf}
\end{center}
From this perspective it isn't so surprising that quantisation prescriptions aren't universal maps between classical systems and quantum systems, i.e., in mathematical language, \emph{functors}. This is an important observation because it is the first serious sign that there is some room to play with in finding quantum field theories: if we are looking for a quantum system with a specific effective classical description in the presence of decoherence there are many answers that will lead to equivalent results, giving us more room to find one with a useful parsimonious description. 

\section{What is a field theory}
Let's now begin our discussion proper by contemplating, at a purely heuristic level, what a \emph{field theory} is meant to be. On a purely intuitive level, a \emph{field} (either quantum or classical) is supposed to comprise of \emph{continuously} many degrees of freedom, i.e., roughly speaking, there is a degree of freedom for each point in space $\mathbb{R}^d$ (or spacetime $\mathbb{R}\times \mathbb{R}^d$). When dealing with such a vast abundance of degrees of freedom the task of just specifying a state of such a field becomes deeply nontrivial. 

Classically, this task can be largely thought of as being solved by calculus. Here \emph{pure field states} can be simply defined to be continuous functions $\phi:\mathbb{R}^d\rightarrow \mathbb{R}$. The space $C(\mathbb{R}^D)$ (or, in the case of spacetime, $C(\mathbb{R}\times\mathbb{R}^D)$) of all \emph{mathematically} possible such field states is rather wild: it is uncountably infinite. Worse, the task of understanding probability measures on such a space is deeply nontrivial, meaning that a statistical theory of classical fields is already rather hard.

Of course, not all of the states in $C(\mathbb{R}^D)$ are meant to be physically realisable, i.e., we haven't yet specified the conditions a \emph{physical state} must satisfy. Classically this is done by requiring that physical pure states satisfy certain differential equations. For example, in a $(1+1)$-dimensional spacetime of points $(t,x)\in \mathbb{R}^2$ we could demand that valid physical states satisfy
\begin{equation}
	\frac{\partial^2\phi}{\partial t^2} - \frac{\partial^2\phi}{\partial x^2} + m^2\phi = 0.
\end{equation}
Just to be a solution of such an equation of motion implicitly requires that $\phi$ is at least twice differentiable. (Leaving aside, for the moment, the topic of weak solutions and distributions.) The condition of differentiability is actually rather nontrivial as it means that at small scales any solution is quite boring (it is basically a straight line) when we zoom in:
\begin{center}
\includegraphics{difffunc.pdf}
\end{center}
While this doesn't help us solve the problem of building statistical theories of classical fields, it does at least allow us to tame the problem of understanding state spaces for systems of continuously many classical degrees of freedom. 

But what about quantum theories? Here we encounter a fundamentally new problem not present in the classical case: it is now hard to even define \emph{pure} field states. Naively this should be straightforward: just define the space of pure states to be the tensor product
\begin{equation}
	\mathcal{H} \text{``$=$''} \bigotimes_{x\in \mathbb{R}^d} \ell^2(X),
\end{equation}
where $\ell^2(X)$ is some hilbert space of functions on a single particle configuration space $X$. The scare quotes here are intended to indicate that the object on the right hand side does not \emph{naively} exist in any satisfying mathematical way\footnote{It does exist in a less naive way via von Neumann's incomplete tensor product. The incomplete tensor product admits an attractive physical intepretation: it is the separable hilbert space describing the configurations of at most a finite number of particles above the vacuum state. Finite can mean, e.g., $10^{34}$.}. But let us be bold physicists for a moment and simply pretend that mathematicians will sort it all out and the space does exist. In this case we ought to have a basis of pure states labelled, e.g., by continuous functions
\begin{equation}
	\{|\phi\rangle \}_{\phi:\mathbb{R}^d\rightarrow \mathbb{R}}.
\end{equation}
While this initially looks reasonable we quickly see that there is a new difficulty: what superpositions are we going to allow? All of them? Surely not: we must find equations that specify for us the physically realisable states. Here we can no longer take recourse to calculus for help. Indeed, the problem of specifying physically realisable pure states of quantum fields is intimately tied to the problem of writing probability measures for classical fields in one lower spatial dimension (via the so called ``classical-quantum'' correspondence). 

In both the classical statistical field and the quantum field cases we have come up against a fundamental physical problem (as opposed to a technical mathematical problem), namely that of specifying interesting states of fields (in the former case, as probability measures, and in the latter as pure states). What we ideally want is a physical principle that tells what are the ``good'', or \emph{physical}, field states versus the ``bad'', or \emph{unphysical}, states.  

\section{Effective field theories}

There is a wonderfully simple and familiar physical argument for why fields should give a good description of the complex behaviour of complex many body systems. Suppose we have some extraordinarily complicated system of many particles of many types: a good example to keep in mind is \emph{water}. Now if it were easy for us to make any conceivable measurement allowed by quantum mechancis of the system \emph{at no cost}, then there is \emph{no way} we'd be fooled into thinking water is anything other than a collection of a vast number of fundamental particles, quarks, gluons, etc., in some incredibly complicated evolving entangled state. The only reason we don't make this description is that we simply \emph{can't} make any measurement of the system without paying some kind of bill: the more complicated the measurement the more we have to pay. Thus we have to settle with making measurements of simpler quantities. For example, our eyes are basically a pair of pretty crappy photon detectors and thus when we look at a water sample we are simply carrying out a very noisy and inefficient POVM. Now here is the main point: when you only have access to a handful of observables then you can formulate a \emph{simpler hypothesis} which can still explain all the observational data you've obtained. This simpler hypothesis is an \emph{effective theory} for the system. Simpler here can mean many things, but in the context of this paper it is via a field theory\footnote{Why are fields simple? The answer is calculus: it is often easier to calculate integrals than sums.}.

A convenient way to describe the large-scale degrees of freedom that humans in their finite world can access is by simply zooming out. How can one, in general, implement the operation of ``zooming out''? Since zooming out corresponds to a physical operation, the answer is that it must be via an irreversible CP map $\mathcal{E}$. The reason that it has to be irreversible is that it must prevent us from measuring degrees of freedom that we would otherwise be able to measure: after all, if we could measure all the observables after zooming out that we could measure before then in what sense can we be have said to have zoomed out? In the context of lattice systems there is a very convenient way to implement the zoom out operation, namely, via Kadanoff blocking. This is the CP map whereby a block of spins is mapped to single spin via the partial trace channel.

One aspect of this discussion may be puzzling for readers familiar with quantum field theories: why are we insisting on saying a change of scale is a lossy operation when, e.g., in CFTs a scale change is a \emph{reversible unitary operation}? The answer is that, in terms of a \emph{microscopic theory} with a cutoff, zooming out \emph{must} be a lossy operation as we can push degrees of freedom past the cutoff. However, in terms of an effective theory, a scale change can be unitary because there is an effective decoupling of the large-scale  and the small-scale degrees of freedom and the action of a finite scale change doesn't, \emph{in the large cutoff limit}, couple the different sets of degrees of freedom. If you like, effective theories are analogous to (approximate) decoherence-free subspaces/subsystem codes, where the errors in this case are induced by scale changes. This is a quantitative way to understand what is meant by IR/UV decoupling.

If zooming out is a CP map $\mathcal{E}$ then what is ``zooming in'', or \emph{UV completion}? The answer is that there is no unique answer: any CP map $\mathcal{D}$ such that $\mathcal{D}\circ \mathcal{E} \equiv \mathcal{I}$ can be given the interpretation as a zooming in map. Exploiting once more the analogy with QECCs you can think of zooming out as an error recover operation $\mathcal{D}$ on the large-scale degrees of freedom. We will return to this point later.


\section{The Wilsonian formulation of effective quantum field theory}
Here we review the Wilsonian view of quantum field theory as an effective field theory. It is \emph{not at all} necessary to understand the discussion here at anything but a metaphoric level as we are going to revisit this section many times in the sequel to make the intuitive discussions here rigourous. Just read through the description and try and absorb the general flavour. 

In the Wilsonian view we begin with the space $\mathcal{A}_{\text{reg}}$ of \emph{regulated} or \emph{cutoff} quantum field theories. What are these? Well, there are an infinite number of ways to define them, but one you can keep in mind is by simply putting the theory on a lattice. So you can think of $\mathcal{A}_{\text{reg}}$ as the space of observables for all theories of bosons and fermions hopping on lattices with some \emph{nonzero} lattice spacing $a = 1/\Lambda$. (This way of defining a cutoff QFT is certainly not the most elegant, but it does enjoy considerable advantages for evaluation on a computer.) Thus $\mathcal{A}_{\text{reg}}$ is the infinite-dimensional space of all lattice theories for all lattice spacings. For each $\Lambda$ there is a subspace $\mathcal{A}_\Lambda$ corresponding to the space of lattice theories with a cutoff given by $\Lambda$, and these are all identified with their image in $\mathcal{A}_{\text{reg}}$ via some \emph{embedding} map $f_\Lambda:\mathcal{A}_\Lambda\rightarrow \mathcal{A}$. Once you have cut off a quantum field theory most, if not all, of the nasty divergences you've heard about disappear, and we only have to cope with more pedestrian divergences familiar to condensed matter physicists (e.g., the ``orthogonality catastrophe'' and ``infrared divergences''). 

One might be tempted to think that $\mathcal{A}_{\text{reg}}$ is already the space of quantum field theories. However, this is really not the case: any resident in the space $\mathcal{A}_{\text{reg}}$ always has, by construction, \emph{finite} cutoff $\Lambda$. This cutoff can be arbitrarily small yet it must always be finite and thus it always has a lattice structure (be it in momentum or position space or otherwise). A good analogy to keep in mind here is that between the rational numbers $\mathbb{Q}$ and the real numbers $\mathbb{R}$: any element of $\mathbb{Q}$ is expressible as $a/b$ with $a<\infty$ and $b < \infty$. However, something like $\sqrt{2}$ would need to be expressed as a rational number with infinitely large numerator and denominator, i.e., there are many numbers `missing' from $\mathbb{Q}$. 

Hence we now imagine that $\mathcal{A}_{\text{reg}}$ lives inside some even larger space $\mathcal{A}$ of ``all'' quantum field theories (whatever that might mean), with or without regulator. This is rather vague, but we argue below that we can ignore almost all the theories in $\mathcal{A}\setminus\mathcal{A}_{\text{reg}}$, as only a small fraction are \emph{physically relevant}.

What we are now going to do is construct proper quantum field theories, i.e., theories without cutoff. To do this suppose we have a theory with cutoff $\Lambda$ and we find to a \emph{physically equivalent} theory with a larger cutoff $\Lambda' > \Lambda$. If we can always do this then there is nothing stopping us sending the cutoff $\Lambda'\rightarrow \infty$ and calling the result a quantum field theory proper. So let's try and make this more concrete. Now what does it mean for a theory to have a larger cutoff than another theory? One clean interpretation is that all the \emph{observables} and \emph{effects} of our original theory can be found in the new theory with a larger cutoff. Thus, corresponding to the operation of changing cutoff from $\Lambda$ to $\Lambda' > \Lambda$, there must be a map
\begin{equation}
	\mathcal{F}_{\Lambda,\Lambda'}: \mathcal{A}_\Lambda \rightarrow \mathcal{A}_{\Lambda'}
\end{equation}
which identifies the effects of the lower-cutoff space with corresponding \emph{physically identical} effects in the higher-cutoff space. 
How do we determine this map? The answer is that the \emph{low-order correlation functions of large-scale low energy observables} need to be preserved under the cutoff changing operation. In other words: if we make a prediction for low-energy large-scale observables using a theory with cutoff $\Lambda$ then an \emph{equivalent} theory in $\mathcal{A}_{\Lambda'}$ must give the same predictions for the original low-energy large scale observables. Because the cutoff can be changed continuously we generate a \emph{flow} on $\mathcal{A}_{\text{reg}}$ (and hence on $\mathcal{A}$) via the map
\begin{equation}
	f_\Lambda\circ \mathcal{F}_{\Lambda,\Lambda'}\circ f_\Lambda^{-1}   
\end{equation}   
according to the diagram
\begin{equation}
\xymatrix{
\mathcal{A}_{\Lambda} \ar[rr]^{\mathcal{F}_{\Lambda,\Lambda'}}
\ar[dr]_{f_\Lambda}\hole
&& \mathcal{A}_{\Lambda'} \ar[dl]^{f_\Lambda'}\\
& \mathcal{A} }
\end{equation}
It is usually assumed  that that this flow on the infinite dimensional space $\mathcal{A}$ is generated by a \emph{vector field}. A very special role is played by the \emph{fixed points} of this flow, as they correspond to genuine cutoff-free quantum field theories, i.e., theories of continuously many degrees of freedom. Conformal field theories, being scale invariant, are precisely fixed points.

It is standard to parametrise QFTs by \emph{local lagrangians} which basically amounts to the choice of a coordinate system for our mythical $\mathcal{A}$. Doing things this way conflates both the states and the effects into one object via the \emph{path integral}. The space $\mathcal{M}$ of lagrangians is an infinite-dimensional linear manifold with a coordinate for each local term you can add, e.g., 
\begin{equation}
	\mathcal{L} = c_0 + c_1\phi + c_2\phi^2 + c_3\phi^2 + \cdots + d_0 \phi \square \phi + d_1 \partial_\mu \phi \partial^\mu \phi +\cdots+ \text{etc.}
\end{equation}
Here a QFT is parametrised by the infinite list $(c_0, c_1, c_2, \ldots, d_0, d_1, \ldots, \text{etc.}) \in \mathbb{R}^\infty$. Lagrangians are, in combination with the path integral, a wonderfully compact way specify QFTs in a way that makes locality, causality, and symmetries such as Lorentz invariance manifest. However, they have the not inconsiderable downside that it is often very hard to compute correlation functions except perturbatively around special points.  

When we do things this way the maps $f_\Lambda$ and $\mathcal{F}_{\Lambda,\Lambda'}$ become \emph{diffeomorphisms} and the requirement that, after increasing the cutoff from $\Lambda$ to $\Lambda'$, the $n$-point correlation functions of large-scale low-energy observables remain invariant generates a deeply nontrivial \emph{renormalisation group} (RG) flow on the infinite dimensional manifold $\mathcal{M}$.  

The point of view taken in this paper is to describe how to apply the Wilsonian view to different, indeed arbitrary, ways of parametrising QFTs. A key step is to first separating out states and observables into separate categories.

As a crucial example, we'll show how to exploit \emph{tensor network states} to parametrise QFTs by implementing the continuum limit in the Wilsonian view directly on quantum states rather than lagrangians.


\section{Effective quantum field states and the Wilsonian formulation}

We hereby attempt to formulate the most economical and conservative abstract interpretation of the Wilsonian view possible while preserving as much of the crucial physical structure as possible.

Casting an eye over the discussion in the previous section we isolate three core components:
\begin{enumerate}
	\item A space of observables $\mathcal{A}_{\text{reg}}$ for \emph{regulated} theories which are mathematically well behaved. The space $\mathcal{A}_{\text{reg}}$ is naturally parametrised with a \emph{cutoff} $\Lambda$, so that we can identify in $\mathcal{A}_{\text{reg}}$ the subspace $\mathcal{A}_{\Lambda}$ of theories with cutoff $\Lambda$.
	\item A way to \emph{compare} the predictions of two theories with \emph{different} cutoffs -- an \emph{information metric}.
	\item A way to identify the ``large-scale'' observables in $\mathcal{A}_{\text{reg}}$.
\end{enumerate}  
As we argue below, once we've specified this ``minimal data'' we can exploit the Wilsonian view to define what is meant by a quantum field state.  
   

Once we've specified these three things we have enough information to define what is meant by a QFT. To motivate our eventual construction we first review the process of \emph{completion}.

\subsection{Regulated observables}

Two examples here: the lattice and momentum cutoff. Need to compare theories with different cutoff. ``Kadanoff'' as the answer. 


Before we begin our argument it is helpful to pause for a moment to digest the three core components of the Wilsonian view. Let's begin by discussing the space $\mathcal{A}_{\text{reg}}$ of regulated theories. What is this? It turns out that there is a large degree of arbitrariness here in the definition of $\mathcal{A}_{\text{reg}}$, depending on the choice of your favourite regulator. We can abstract away some of this by specifying some requirements. The key insight here is that we must have a \emph{partial order} on the way you regulate a theory. Here are two key examples: (a) the \emph{momentum-space regulator}, here we simply restrict all degrees of freedom to have momenta less than $\Lambda$. In this example the regulator is the cutoff $\Lambda$ -- note that there is a way to say that one cutoff is larger than another; (b) the \emph{lattice regulator}, which is given by a partition of space into a bunch of cells and we associate a degree of freedom with each cell. Here the regulator is specified by a \emph{partition} which, in the case of non regular discretisations, is no longer a simple number. However, there is still a \emph{partial order} on the space of partitions because there is a way to say that one partition is \emph{finer} than another. The abstract structure at play here is a bunch of mathematically well-defined theories $\mathcal{A}_\Lambda$ indexed by a set $I$ of ``cutoffs'' which has a \emph{partial order}. The space $\mathcal{A}_{\text{reg}}$ is then the smallest space which includes all of the effects $\mathcal{A}_\Lambda$, for all $\Lambda \in I$. If we don't have $\mathcal{A}_{\text{reg}}$ from the outset then we can always build it by taking the \emph{direct sum} of all the $\mathcal{A}_\Lambda$s and then identifying effects in all the different $\mathcal{A}_\Lambda$s which correspond to identical experiments (more on this later). Thus 
\begin{equation}
	\mathcal{A}_{\text{reg}} \equiv \left(\bigoplus_{\Lambda \in I} \mathcal{A}_\Lambda\right)\Bigg/\sim.
\end{equation} 
The space $\mathcal{A}_{\text{reg}}$ \emph{isn't yet} the space of QFTs\footnote{Following a suggestion of Witten, Moore calls $\mathcal{A}_{\text{reg}}$ the space of \emph{weak QFTs}, by analogy with the space of weak solutions to a PDE.}.


\subsection{Distance measures}

Review some QI distance measure stuff.


The next component we need to understand is how to \emph{compare} the predictions of two theories with differing cutoffs. This requires the specification of (a family of) \emph{distance measures} or \emph{information metrics}. This can be found by first agreeing on an operationally motivated distance measure on the \emph{state spaces} $\mathcal{S}_\Lambda$ of the $\mathcal{A}_\Lambda$s, and then extending this to the state space $\mathcal{S}_{\text{reg}}$ of all regulated states on $\mathcal{A}_{\text{reg}}$ in a way compatible with the equivalence relation $\sim$. This can be nontrivial; we'll discuss this at length in the following sections for the two important examples of the momentum cutoff and the lattice discretisation.


\subsection{Large-scale observables}


The final component is to specify the ``large-scale observables'' in $\mathcal{A}_{\text{reg}}$. This is done by specifying a \emph{completely positive map} $\mathcal{E}_\sigma : \mathcal{A}_{\text{reg}} \rightarrow \mathcal{A}_{\text{reg}}$, parametrised by one or more \emph{resolution} or \emph{scale parameters} $\sigma$, whose image is the convex set of all those effects measurable by large-scale observers (i.e., those measurement that we mere mortals can perform). 

\section{Quantum field states via completion}
Let's tell a story about how physicists on a world far far away might have invented the real numbers $\mathbb{R}$.  The scientists of this world wanted to measure distances using rulers with tick marks spaced at regular intervals. On this world it was agreed that all rulers had to have their ticks spaced by fractions of some standard tick length $a_0$ (the length of the emperor's foot). Thus the tick spacing $\epsilon a_0$ of a ruler was required to be a positive rational ratio $\epsilon \in \mathbb{Q}^+$ of the standard tick length $a_0$. Owing to its canonical nature $a_0$ was set to $1$ by convention. To the scientists of this world all lengths were rational numbers. 

Now a scientist in possession of a ruler of accuracy $\epsilon$ would deem two lengths $x$ and $y$ differing by $\epsilon$ to be \emph{indistinguishable} (they rounded everything down on this world), i.e., two points $x$ and $y$ were \emph{$\epsilon$ equivalent} if
\begin{equation}
	x\sim_\epsilon y \Leftrightarrow |x-y| < \epsilon. 
\end{equation} 
For the longest time it was assumed that all lengths had rational values, after all, you could always build a better ruler to measure a length more accurately. 

However, one day it came to pass that a radical, clever, and sadly, unfortunate, scientist on this world made a shocking realisation: there could be a length $x^\star = \sqrt{2}$ which was \emph{not} exactly a rational number! Of course, this poor scientist was summarily executed (there is nothing worse than a smart ass). However, the argument gnawed at the minds of the conservative establishment for decades, and much hot air expended in coming to terms with it. Eventually, the inhabitants of this world came to the following construction. Obviously noone on the planet could measure the length $x_{\epsilon} = n\epsilon$ to be anything other than a multiple $n$ of the ticklength $\epsilon$ found from $n = \lfloor x^\star/\epsilon \rfloor$. However, it could obviously be the case that after the installation of a ruler upgrade to ticklength $\epsilon'$ the newly measured length $x_{\epsilon'} \equiv n'\epsilon$, where $n' = \lfloor x^\star/\epsilon \rfloor$ would \emph{change} as it would be more accurate. Now, supposed the theorists of this world, what if we wanted a specification of a length that would encode the result of a measurement by a ruler with arbitrarily good (but still finite!) ticklength  $\epsilon > 0$ (infinite accuracy was frowned upon)? Well this was obvious: just specify all possible lengths by \emph{sequences} of successively more accurate lengths:
\begin{equation}
	(x_n)_{n \in \mathbb{N}}, \quad x_n\in \mathbb{Q}^+.
\end{equation}
Now not all sequences were allowed, only those which became more accurate as $n \rightarrow \infty$. That is, only those sequences $(x_\epsilon)_{n \in \mathbb{N}}$ enjoying the property that $\forall \epsilon, \exists N\in \mathbb{N}$ such that $\forall m,n\in \mathbb{N}$
\begin{equation}
	|x_n-x_m| < \epsilon,
\end{equation} 
where considered to correspond to a possible length. It was clear that all the originally accepted lengths $x\in \mathbb{Q}^+$ admitted the representation $(x_n)_{n\in \mathbb{N}}$, with $x_n = x$, so this new definition could accommodate the old one. The space of all these sequences was denoted $\widehat{\mathbb{R}}^+$. However, it was now possible that two apparently different sequences would correspond to the same length. This was solved by introducing the equivalence relation 
\begin{equation}
	(x_n)_{n \in \mathbb{N}} \sim (y_n)_{n \in \mathbb{N}} \Leftrightarrow \text{if $\forall \epsilon$, $\exists N\in \mathbb{N}$ such that $m,n>N$ it was true that $|x_n-y_n| < \epsilon$.} 
\end{equation}
Thus, the space of all possible lengths was defined to be $\mathbb{R}^+ \equiv \widehat{\mathbb{R}}^+/\sim$.

The story of how they discovered negative lengths must be told elsewhere...

Of course this fairy tale is just an allegory for the construction of the real numbers by the process of \emph{completion} of the rational numbers, whereby ``continuous'' things (i.e., elements of $\mathbb{R}$) are constructed by sequences of ``not-so-continuous'' things (i.e., the positive ratioanl numbers)

Let's isolate two key features of this allegory. Firstly, we have the space of ``not-so-continuous'' \emph{regularised} objects which specify objects with \emph{finite accuracy}. Secondly, we have a way to talk about the \emph{precision}, by understanding when two objects are effectively \emph{indistinguishable}. We then build the continuous objects as sequences of finite-accuracy objects modulo the equivalence relation that two such sequences represent the same object if they are indistinguishable for all accuracies.

\subsection{What a field state is}
By analogy with the construction of the rational numbers in the previous section we now describe how to define a \emph{quantum field state}. What we need 
\begin{enumerate}
	\item Define a set $\mathcal{F}$ of \emph{regularised} field states which specify the field state to some finite accuracy (these states are analogous to the positive rationals $\mathbb{Q}^+$); and
	\item define \emph{accuracy}, i.e., a way of saying if two regularised field states $\rho$ and $\rho'$ are effectively indistinguishable given certain idealised experimental limitations specified by a list of numbers, called \emph{resolutions}, $r \equiv (\sigma, h, \ldots)$. (This requirement is analogous to writing $|x-y|<\epsilon$ for the positive rationals.)
\end{enumerate}


\section{Building Cauchy sequences: the renormalisation group}

We've learnt that the building block of a quantum field state is a \emph{Cauchy sequence} of regulated quantum field states where we build a metric by insisting that the large-scale observable properties are close. This is all well and good but we now need to argue that such sequences exist. This is the task of this section where we argue that not only such sequences exist but also there are an embarrassment of riches of them. We then argue that this plentitude can cut down to a manageable size by imposing a simple hypothesis for their structure

Recall that our large-scale observables at scale $\sigma$ are determined in the Heisenberg picture by a CP map $\mathcal{E}_\sigma$ which acts by embedding or ``spreading out'' the observables across the UV degrees of freedom. In the Schr\"odinger picture this CP acts dually as $\mathcal{E}_\sigma^*$ where it is a \emph{coarse graining} map that essentially acts as a partial trace. Indeed, in the case of the hard momentum cutoff it is precisely a partial trace over the large momentum degrees of freedom.  Now, suppose we have some reduced density operator $\rho_\sigma$ from which we'd like to build a Cauchy sequence, i.e., from which we'd like to remove the UV regulator. To do this we need to ``put back in'' the state on the degrees of freedom between $\sigma$ and a UV cutoff $\Lambda$ in a way that would be consistent for all $\sigma < \Lambda$. This is the same as looking for a sequence $(\rho_{\Lambda_n})_{n=1}^\infty$ of states with the property that
\begin{equation}
\rho_{\Lambda_m} = \tr_{(\Lambda_m,\Lambda_n]}(\rho_{\Lambda_n}), \quad \Lambda_m < \Lambda_n.
\end{equation}
I.e., the state $\rho_{\Lambda_n}$ must be an \emph{extension} or, improperly, a \emph{purification}, of $\rho_{\Lambda_m}$ onto the larger bipartite system comprising the original interval $\Lambda_m$ and the complement of $\Lambda_m$ in $\Lambda_n$. Now there are an infinite number of extensions/purifications for any given state, so there are an infinite number of possible sequences. These are all Cauchy. In fact, even better, they are \emph{exactly} convergent sequences! 
What is happening here is that we are finding the \emph{inverse} of a many to one map (the coarse graining map $\mathcal{E}_\sigma$). Because the map is many to one there are, by definition, many inverses, each of which has every right to be called a UV completion or continuum limit. What an embarrassment of riches! 

We have every right to be dissatisfied and suspicious with the proposed solution above. As the astute reader will no doubt observe: the sequences $(\rho_{\Lambda_n})_{n=1}^\infty$ are not useful for physics because how could we ever decide which one is the correct one by doing experiments? Well, you know what, tough luck: to quote Polchinski, ``nobody ever promised you a rose garden''. This is just the way things are: it is simply the case that something bizarre and crazy could turn up at any moment in our particle accelerators as we upgrade them to access more an more observables. There is no mathematical proof that could ever exclude this possibility apart from our desire for the universe to be kind to us and be explainable. You just have to learn to live with this. 

The way we implement the constraint of ``explainability'' is to demand that our states are, at least in principle, \emph{parametrisable} in terms of a finite number of numbers, or \emph{coupling constants}. Traditionally this is carried 

\begin{center}
\includegraphics{rginverse.pdf}
\end{center}



We then define a \emph{field state} to be a sequence\footnote{More generally we'll need to specify field states by \emph{nets} $\phi_\Lambda \in \mathcal{F}$, $\Lambda\in\mathbb{R}^+$.} $\phi_n\in\mathbb{F}$, $n\in\mathbb{N}$, of field states such that for all values of the resolution parameters $r$, $\exists N\in \mathbb{N}$ such that $\forall m,n > N$ the states $\phi_m$ and $\phi_n$ are effectively indistinguishable at resolution $r$. Thus we have come to the idea of a \emph{Cauchy sequence} of regularised field states. The process of finding a Cauchy sequence, i.e., of building a sequence such that $\phi_n$ is Cauchy, is known as the \emph{renormalisation group}.

\section{Discussion}




\bibliography{What-is-a-quantum-field-state}

%\appendix


\end{document}  


